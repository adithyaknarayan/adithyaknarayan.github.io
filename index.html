<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Adithya Narayan </title> <meta name="author" content="Adithya Narayan"> <meta name="description" content="MSc in Computer Vision at Carnegie Mellon University. Research interests: Computer Vision, AI, and Computer Graphics. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta property="og:site_name" content="Adithya Narayan"> <meta property="og:type" content="website"> <meta property="og:title" content="Adithya Narayan | about"> <meta property="og:url" content="https://adithyaknarayan.github.io/"> <meta property="og:description" content="MSc in Computer Vision at Carnegie Mellon University. Research interests: Computer Vision, AI, and Computer Graphics. "> <meta property="og:image" content="https://adithyaknarayan.github.io/assets/img/prof_pic.jpg"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="about"> <meta name="twitter:description" content="MSc in Computer Vision at Carnegie Mellon University. Research interests: Computer Vision, AI, and Computer Graphics. "> <meta name="twitter:image" content="https://adithyaknarayan.github.io/assets/img/prof_pic.jpg"> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Adithya Narayan"
        },
        "url": "https://adithyaknarayan.github.io/",
        "@type": "WebSite",
        "description": "MSc in Computer Vision at Carnegie Mellon University. Research interests: Computer Vision, AI, and Computer Graphics.
",
        "headline": "about",
        
        "sameAs": ["https://scholar.google.com/citations?user=hN3nsd4AAAAJ", "https://github.com/adithyaknarayan", "https://www.linkedin.com/in/adithya-n-b637b8146"],
        
        "name": "Adithya Narayan",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://adithyaknarayan.github.io/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Adithya Narayan </h1> <p class="desc">MSc in Computer Vision @ Carnegie Mellon University · Human Sensing Lab · Pittsburgh, PA<br> <a class="btn btn-sm btn-outline-primary" href="/cv/">CV</a> <a class="btn btn-sm btn-outline-primary" href="https://scholar.google.com/citations?user=hN3nsd4AAAAJ&amp;hl=en" target="_blank" rel="noopener">Scholar</a> <a class="btn btn-sm btn-outline-primary" href="https://github.com/adithyaknarayan" target="_blank" rel="noopener">GitHub</a> <a class="btn btn-sm btn-outline-primary" href="mailto:anaraya2@cs.cmu.edu">Email</a> </p> </header> <article> <div class="profile float-right"> <figure> <picture> <img src="/assets/img/prof_pic.jpg?c5432a87381c6bdc1e69911305fa9a1d" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="clearfix"> <p>I’m Adithya Narayan, a Graduate Research Assistant at the <a href="http://www.humansensing.cs.cmu.edu/" rel="external nofollow noopener" target="_blank">Human Sensing Lab</a>, advised by <a href="http://www.cs.cmu.edu/~ftorre/" rel="external nofollow noopener" target="_blank">Prof. Fernando De la Torre</a>, and an MSCV student at the <a href="https://www.ri.cmu.edu/" rel="external nofollow noopener" target="_blank">Robotics Institute</a> at <a href="https://www.cmu.edu/" rel="external nofollow noopener" target="_blank">Carnegie Mellon University</a>.</p> <p>My research interests center around <strong>3D vision</strong> and <strong>geometry-aware learning</strong>—especially multi-view reasoning, robust reconstruction, and understanding how vision(-language) models build 3D scene representations.</p> <p>Currently, I’m exploring:</p> <ul> <li> <strong>Multi-view reasoning / view selection</strong> for 2D VLMs, and how 3D understanding emerges (Gaussian Splatting + depth representations; mechanistic interpretability).</li> <li> <strong>Adversarial scene exploration</strong> on SE(3) with ordinal objectives to expose geometry/depth failure modes (CVPR 2026, under review).</li> </ul> <p>Previously, I’ve worked across research engineering and applied ML:</p> <ul> <li> <strong>HeyGen (Research Engineering Intern)</strong>: camera-motion conditioned video diffusion (ControlNet), large-scale SfM + pose extraction, and data filtering with flow-based signals.</li> <li> <strong>Arintra (ML Engineer)</strong>: RAG for medical coding, semantic retrieval (SapBERT + Qdrant), and ML deployment/versioning (MLFlow + FastAPI + GCP).</li> <li> <strong>Klothed (ML Engineer)</strong> (Advisor: <a href="http://obrien.berkeley.edu/" rel="external nofollow noopener" target="_blank">Prof. James O’Brien</a>): 3D human reconstruction and fast FEM-based warping pipelines for AR.</li> <li> <strong>Origin Health (Research Engineer)</strong> (Advisor: Dr. Sripad Devalla): fetal imaging systems (segmentation / measurement), including a co-authored ISBI 2022 paper.</li> </ul> <p>I completed my undergrad at Manipal Institute of Technology (B.Tech ECE) in 2021.</p> <hr> <h2 id="timeline">Timeline</h2> <figure> <picture> <img src="/assets/img/timeline.png" class="img-fluid" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> My journey so far (research + industry). </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> </div> <div id="narayan2026breaking" class="col-sm-8"> <div class="title">Breaking Depth Estimation Models with Semantic Adversarial Attacks</div> <div class="author"> <em>Adithya Narayan</em>, Ujjwal Ojha , Jonas Theiss , and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Aditya Prakash, Fernando Torre' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2026</em> , 2026 </div> <div class="periodical"> Under Review </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/placeholder" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We introduce a systematic toolbox for diagnosing monocular depth estimation models by discovering failure cases arising from camera viewpoint changes. Our method employs a differentiable rendering pipeline to adversarially optimize camera extrinsics, sampling semantically meaningful but challenging viewpoints. We construct a dataset of complex 3D indoor scenes and evaluate four state-of-the-art depth estimation models, showing that our framework exposes consistent weaknesses and establishes a new benchmark for robustness analysis.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Towards-a-device-independent-deep-learning.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Towards-a-device-independent-deep-learning.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lad2022towards" class="col-sm-8"> <div class="title">Towards a device-independent deep learning approach for the automated segmentation of sonographic fetal brain structures: a multi-center and multi-device validation</div> <div class="author"> Abhi Lad , <em>Adithya Narayan</em>, Hari Shankar , and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Shefali Jain, Pooja Punjani Vyas, Divya Singh, Nivedita Hegde, Jagruthi Atada, Jens Thang, Saw Shier Nee, others' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>In Medical Imaging 2022: Computer-Aided Diagnosis</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2202.13553" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In this study, we propose a DL based segmentation framework for the automated segmentation of 10 key fetal brain structures from 2 axial planes from fetal brain USG images (2D). We developed a custom U-Net variant that uses inceptionv4 block as a feature extractor and leverages custom domain-specific data augmentation. Quantitatively, the mean (10 structures; test sets 1/2/3/4) Dice-coefficients were: 0.827, 0.802, 0.731, 0.783. Irrespective of the USG device/center, the DL segmentations were qualitatively comparable to their manual segmentations. The proposed DL system offered a promising and generalizable performance (multi-centers, multi-device) and also presents evidence in support of device-induced variation in image quality (a challenge to generalizibility) by using UMAP analysis. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/Leveraging-clinically-relevant.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Leveraging-clinically-relevant.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="shankar2022leveraging" class="col-sm-8"> <div class="title">Leveraging clinically relevant biometric constraints to supervise a deep learning model for the accurate caliper placement to obtain sonographic measurements of the fetal brain</div> <div class="author"> Hari Shankar , <em>Adithya Narayan</em>, Shefali Jain , and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Divya Singh, Pooja Vyas, Nivedita Hegde, Purbayan Kar, Abhi Lad, Jens Thang, Jagruthi Atada, others' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>In 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI)</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2203.14482" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We leveraged clinically relevant biometric constraints (relationship between caliper points) and domain-relevant data augmentation to improve the accuracy of a U-Net DL model (trained/tested on: 596 images, 473 subjects/143 images, 143 subjects). We performed multiple experiments demonstrating the effect of the DL backbone, data augmentation, generalizability and benchmarked against a recent state-of-the-art approach through extensive clinical validation (DL vs. 7 experienced clinicians). For all cases, the mean errors in the placement of the individual caliper points and the computed biometry were comparable to error rates among clinicians. The clinical translation of the proposed framework can assist novice users from low-resource settings in the reliable and standardized assessment of fetal brain sonograms. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <img src="/assets/img/publication_preview/firstauthorISUOG.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="firstauthorISUOG.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="narayan2021oc11" class="col-sm-8"> <div class="title">OC11. 02: A multicentre, multi-device validation of a deep learning system for the automated segmentation of fetal brain structures from two-dimensional ultrasound images.</div> <div class="author"> <em>A. Narayan</em>, S Kaushik , H Shankar , and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'S Jain, N Hegde, P Vyas, J Atada, SP Manjushree, J Thang, S Saw, others' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>Ultrasound in Obstetrics &amp; Gynecology</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://obgyn.onlinelibrary.wiley.com/doi/full/10.1002/uog.23853" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We retrospectively obtained 4,190 two-dimensional (2D) ultrasonography (USG) images (1349 pregnancies; TV + TC images) from 3 centres (2 tertiary referral centre [TRC 1,2] + 1 routine imaging centre [RIC]) using 6 ultrasound (USG) devices (GE Voluson: P8,P6,E8,E10,S10; Samsung: HERA W10). A custom U-Net was trained (2744 images from TRC 1 [E8, S10]) on 2D fetal brain images (TV + TC images) and their corresponding manual segmentations to segment 10 key fetal structures (TV + TC planes). We assessed the robustness (operator &amp; centre variability) and generalisability (across devices) of the proposed approach across 4 independent (unseen) test sets. Test set 1 (TRC 1, trained devices): 718 images (E8, S10); test 2 (TRC 1, unseen devices): 192 images (HERA W10, P6, E10); test set 3 (TRC 2, trained device): 378 images (E8), and test set 4 (RIC, unseen device): 158 images (P8). The segmentation performance was qualitatively and quantitatively (Dice coefficient [DC]) assessed.</p> </div> </div> </div> </li> </ol> </div> <h2> <a href="/projects/" style="color: inherit">selected projects</a> </h2> <div class="projects"> <div class="project-item" style="display: flex; align-items: flex-start; margin-bottom: 20px;"> <img src="assets/img/projects/AvatarsFTW/animation1.gif" alt="AvatarsFTW: 3D Human Avatars From The Wild" class="project-image" style="max-width: 250px; height: auto; border-radius: 5px; margin-right: 20px;"> <div class="project-details"> <h2>AvatarsFTW: 3D Human Avatars From The Wild</h2> <p style="font-style: italic; margin-top: 5px;"> Authors: <a href="https://adithyaknarayan.github.io/" target="_blank" style="text-decoration: none; text-decoration: underline;">Adithya Narayan</a> , <a href="http://www.kaustavmukherjee.com/" target="_blank" style="text-decoration: none;" rel="external nofollow noopener">Kaustav Mukherjee</a> , <a href="https://www.linkedin.com/in/shaurye-aggarwal-3a8560b9" target="_blank" style="text-decoration: none;" rel="external nofollow noopener">Shaurye Aggarwal</a> </p> <div style="margin-top: 10px; display: flex; gap: 15px;"> <a href="https://www.kaustavmukherjee.com/AvatarsFTWPage/" target="_blank" style="text-decoration: none; display: flex; align-items: center;" rel="external nofollow noopener"> <i class="fa-solid fa-file" style="margin-right: 5px;"></i> Project Page </a> <a href="https://github.com/adithyaknarayan/16-824_F2024_Project" target="_blank" style="text-decoration: none; display: flex; align-items: center;" rel="external nofollow noopener"> <i class="fa-brands fa-github" style="margin-right: 5px;"></i> GitHub Code </a> <a href="https://docs.google.com/presentation/d/1_AaUpezlMnlBJ_BsKAjCmeMBlCq-SAUWENb9bSGP9CM/edit#slide=id.p" target="_blank" style="text-decoration: none; display: flex; align-items: center;" rel="external nofollow noopener"> <i class="fa-brands fa-google" style="margin-right: 5px;"></i> Google Slides </a> <a href="https://www.kaustavmukherjee.com/AvatarsFTWPage/static/pdfs/AvatarFTW_Report.pdf" target="_blank" style="text-decoration: none; display: flex; align-items: center;" rel="external nofollow noopener"> <i class="fa-solid fa-file-pdf" style="margin-right: 5px;"></i> Report </a> </div> <p style="margin-top: 10px; text-align: justify;"> We propose a two-part, inpainting and body fitting pipeline that alleviates 3D human reconstruction issues with human-object interactions, occlusions, and dynamic poses. The inpainting pipeline uses keypoint detection and a novel keypoint estimation technique, uses LaMa for occluding object removal, Stable Diffusion with ControlNets for generation of missing areas, and a GAN inversion step to create a seamless, plausible human reconstruction. The body fitting pipeline uses an improved regressor and adds more losses to the iterative fitting stage to achieve a better human mesh fit in dynamic poses. The figure above demonstrates our work's ability to inpaint human images, generate improved meshes for incomplete images, and fit better human meshes to a variety of highly dynamic poses. </p> </div> </div> <div class="project-item" style="display: flex; align-items: flex-start; margin-bottom: 20px;"> <img src="assets/img/projects/RobustPointTrackingEpipolar/splash_p1.png" alt="Robust Point Tracking with Epipolar Constraints" class="project-image" style="max-width: 250px; height: auto; border-radius: 5px; margin-right: 20px;"> <div class="project-details"> <h2>Robust Point Tracking with Epipolar Constraints</h2> <p style="font-style: italic; margin-top: 5px;"> Authors: <a href="https://adithyaknarayan.github.io/" target="_blank" style="text-decoration: none; text-decoration: underline;">Adithya Narayan</a> , <a href="" target="_blank" style="text-decoration: none;">Tanisha Gupta</a> , <a href="" target="_blank" style="text-decoration: none;">Lamia Alsalloom</a> </p> <div style="margin-top: 10px; display: flex; gap: 15px;"> <a href="https://github.com/adithyaknarayan/EpiTrack" target="_blank" style="text-decoration: none; display: flex; align-items: center;" rel="external nofollow noopener"> <i class="fa-brands fa-github" style="margin-right: 5px;"></i> GitHub Code </a> <a href="/assets/pdf/Geom_Final_report-2.pdf" target="_blank" style="text-decoration: none; display: flex; align-items: center;"> <i class="fa-solid fa-file-pdf" style="margin-right: 5px;"></i> Report </a> </div> <p style="margin-top: 10px; text-align: justify;"> Geometry-driven point tracking that reduces long-horizon drift by enforcing epipolar consistency via post-processing refinement and weakly-supervised finetuning. </p> </div> </div> <div class="project-item" style="display: flex; align-items: flex-start; margin-bottom: 20px;"> <img src="assets/img/projects/MPPI/push_box_linear_k_3.gif" alt="Warm Start and Knot Point Interpolation for Whole Body MPPI" class="project-image" style="max-width: 250px; height: auto; border-radius: 5px; margin-right: 20px;"> <div class="project-details"> <h2>Warm Start and Knot Point Interpolation for Whole Body MPPI</h2> <p style="font-style: italic; margin-top: 5px;"> Authors: <a href="" target="_blank" style="text-decoration: none;">Anoushka Alavilli</a> , <a href="https://adithyaknarayan.github.io/" target="_blank" style="text-decoration: none; text-decoration: underline;">Adithya Narayan</a> , <a href="" target="_blank" style="text-decoration: none;">Alyn Kirsch Tornell</a> , <a href="" target="_blank" style="text-decoration: none;">Colleen Que</a> , <a href="" target="_blank" style="text-decoration: none;">Lamia Alsalloom</a> </p> <div style="margin-top: 10px; display: flex; gap: 15px;"> <a href="https://docs.google.com/presentation/d/1aIJi8TTQ_2apgSVLX4fu_1wvldGSHf7OTNuZTS-WaT4/edit#slide=id.p" target="_blank" style="text-decoration: none; display: flex; align-items: center;" rel="external nofollow noopener"> <i class="fa-brands fa-google" style="margin-right: 5px;"></i> Google Slides </a> <a href="https://drive.google.com/file/d/1oq_rm9oHCNvl-zMzQ6gcCnyWImXM0DPZ/view?usp=sharing" target="_blank" style="text-decoration: none; display: flex; align-items: center;" rel="external nofollow noopener"> <i class="fa-solid fa-file-pdf" style="margin-right: 5px;"></i> Report </a> </div> <p style="margin-top: 10px; text-align: justify;"> We build upon the results from “Real-Time Whole-Body Control of Legged Robots with Model-Predictive Path Integral Control,” by Alvarez-Padilla, et al., which introduces whole-body sampling-based control for locomotion and manipulation on quadrupedal robots. Our contributions include the implementation of warm-starting to speed up the control trajectory optimization process, an ablation of spine interpolation order and knotpoint density, and tests of these improvements on newly designed terrains. We see that warm-starting and changes to spline order can help in some cases, such as stair-climbing and uneven terrain traversal, whereas the nominal implementation of MPPI is best for more basic tasks such as trotting in place. </p> </div> </div> <div class="project-item" style="display: flex; align-items: flex-start; margin-bottom: 20px;"> <img src="assets/img/projects/FaceTracker/full_segment.gif" alt="Simple Face Tracker with Smoothing" class="project-image" style="max-width: 250px; height: auto; border-radius: 5px; margin-right: 20px;"> <div class="project-details"> <h2>Simple Face Tracker with Smoothing</h2> <p style="font-style: italic; margin-top: 5px;"> Authors: <a href="https://adithyaknarayan.github.io/" target="_blank" style="text-decoration: none; text-decoration: underline;">Adithya Narayan</a> </p> <div style="margin-top: 10px; display: flex; gap: 15px;"> <a href="https://github.com/adithyaknarayan/face-track" target="_blank" style="text-decoration: none; display: flex; align-items: center;" rel="external nofollow noopener"> <i class="fa-brands fa-github" style="margin-right: 5px;"></i> GitHub Code </a> </div> <p style="margin-top: 10px; text-align: justify;"> Developed a face tracking system that uses Multi-task Cascaded Convolutional Networks (MTCNN) for face detection and applies trajectory smoothing to keep the tracking stable across video frames. This Python-based tool processes videos to identify and track faces, outputting annotated videos that demonstrate its effectiveness. It's user-friendly, with simple setup instructions and options to customize with your own videos and reference face images, making it versatile for various applications. </p> </div> </div> <div class="project-item" style="display: flex; align-items: flex-start; margin-bottom: 20px;"> <img src="assets/img/projects/SpringElement/triangulation_vis.png" alt="Optimizing Fabrics in 2D" class="project-image" style="max-width: 250px; height: auto; border-radius: 5px; margin-right: 20px;"> <div class="project-details"> <h2>Optimizing Fabrics in 2D</h2> <p style="font-style: italic; margin-top: 5px;"> Authors: <a href="https://adithyaknarayan.github.io/" target="_blank" style="text-decoration: none; text-decoration: underline;">Adithya Narayan</a> </p> <div style="margin-top: 10px; display: flex; gap: 15px;"> <a href="/blog/2023/Spring-System/" target="_blank" style="text-decoration: none; display: flex; align-items: center;"> <i class="fa-solid fa-link" style="margin-right: 5px;"></i> Project URL </a> </div> <p style="margin-top: 10px; text-align: justify;"> Leveraging vectorized operations in Numpy to find a quick solution to optical flow systems in 2D. </p> </div> </div> <div class="project-item" style="display: flex; align-items: flex-start; margin-bottom: 20px;"> <div class="project-details"> <h2>Simple Face Tracker with Smoothing</h2> <p style="font-style: italic; margin-top: 5px;"> Authors: <a href="https://adithyaknarayan.github.io/" target="_blank" style="text-decoration: none; text-decoration: underline;">Adithya Narayan</a> </p> <div style="margin-top: 10px; display: flex; gap: 15px;"> <a href="https://github.com/adithyaknarayan/face-track" target="_blank" style="text-decoration: none; display: flex; align-items: center;" rel="external nofollow noopener"> <i class="fa-brands fa-github" style="margin-right: 5px;"></i> GitHub Code </a> </div> <p style="margin-top: 10px; text-align: justify;"> Developed a way to memorize information between chunks of video generated using a video diffusion model using point-clouds. Using a fast rendering approach, we update the latent chunks to allow for long range video generation without error accumulation. </p> </div> </div> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Dec 29, 2024</th> <td> Will be serving as a reviewer for IJCV 2025! </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 15, 2024</th> <td> Joined the <a href="http://www.humansensing.cs.cmu.edu/people" rel="external nofollow noopener" target="_blank">Human Sensing Lab</a> under Prof. Fernando De la Torre . </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 24, 2024</th> <td> Started the <a href="https://www.ri.cmu.edu/education/academic-programs/master-of-science-computer-vision/" rel="external nofollow noopener" target="_blank">Master of Science in Computer Vision (MSCV)</a> program at Carnegie Mellon University! </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 01, 2022</th> <td> Presenting two papers at <a href="https://spie.org/" rel="external nofollow noopener" target="_blank">SPIE</a> and <a href="https://ieeexplore.ieee.org/xpl/conhome/1000080/all-proceedings" rel="external nofollow noopener" target="_blank">ISBI</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 01, 2021</th> <td> Presenting an oral and poster paper at ISUOG 2021! </td> </tr> </table> </div> </div> <h2> <a href="/blog/" style="color: inherit">latest posts</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Dec 21, 2025</th> <td> <a class="news-title" href="/blog/2025/Robust-Point-Tracking-with-Epipolar-Constraints/">Robust Point Tracking with Epipolar Constraints</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 25, 2023</th> <td> <a class="news-title" href="/blog/2023/Spring-System/">Simulating Fabrics: A Spring Element Approach</a> </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 27, 2018</th> <td> <a class="news-title" href="/blog/2018/Camera-Control-For-Video-Diffusion-Models/">Camera Control for Long Video Generation in Diffusion Models</a> </td> </tr> </table> </div> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%61%6E%61%72%61%79%61%32@%63%73.%63%6D%75.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=hN3nsd4AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/adithyaknarayan" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/adithya-n-b637b8146" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> </div> <div class="contact-note">Best way to reach me is email. </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Adithya Narayan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?5d75c11f89cd96294bf5e6dd1ee1bb30"></script> <script defer src="/assets/js/common.js?fcfacfb8c6281f5e68d5a7d348186eb1"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-F1W6SYWV7R"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-F1W6SYWV7R");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>